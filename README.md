# [CS229 : Machine Learning](http://cs229.stanford.edu/)

<img src="https://github.com/SKKSaikia/CS229_ML/blob/master/img/cs229.jpg">

The "ML" course at Stanford , or to say the most popular Machine Learning course Worldwide is CS229. CS229 is Math Heavy and is ðŸ”¥, unlike a simplified online version at Coursera, "[Machine Learning](https://www.coursera.org/learn/machine-learning)". I [completed](https://www.coursera.org/account/accomplishments/verify/4G25AQXD9LDG) the online version as a Freshaman and here I take the CS229 Stanford version. I have access to the 2013 video lectures of CS229 from [ClassX](http://classx.stanford.edu/) (I downloaded them, while I was a visiting student at Stanford). All in all, we have the slides, notes from the course website to learn the content. Stay truthful, maintain Honor Code and Keep Learning. Learning is a journey! 

No Text , but [Pattern Classification - Richard Duda, Peter Hart and David Stork](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Pattern%20Classification%20by%20Richard%20O.%20Duda%2C%20David%20G.%20Stork%2C%20Peter%20E.Hart%20.pdf) | [handout](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/handout.pdf)

ð“„† <b>Important Books : </b><br/>
ð“Š– [Hands on Machine Learning with Scikit Learn and TensorFlow](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Hands%20on%20Machine%20Learning%20with%20Scikit%20Learn%20and%20TensorFlow_2.pdf) <br/>
ð“Š– [Introduction to Machine Learning - Ethem AlpaydÄ±n](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Introduction%20to%20Machine%20Learning%20Ethem%20Alpayd%C4%B1n_machinelearning_2010.pdf) <br/>
ð“Š– [Machine Learning A Probabilistic Perspective](https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf) <br/>
ð“Š– [Optimization for Machine Learning](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Optimization%20for%20Machine%20Learning%20%5BSra%2C%20Nowozin%20%26%20Wright%202011-09-30%5D.pdf) <br/>
ð“Š– [Pattern Recognition and Machine Learning - Bishop](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) <br/>
ð“Š– [Pattern Recognition and Machine Learning Solution](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Pattern%20Recognition%20and%20Machine%20Learning%20Solution.pdf) <br/>

<b> Homework (40%) + Mid-term (20%) + Final Project (40%) </b>

# [Syllabus](http://cs229.stanford.edu/syllabus.html) | [Projects](http://cs229.stanford.edu/projects.html) 

# ðŸ¥¤ Homeworks:

# Course:

<p align="justify">This course provides a broad introduction to <b>machine learning</b> and <b>statistical pattern recognition</b>. Topics include: <b>supervised learning</b> (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); <b>unsupervised learning</b> (clustering, dimensionality reduction, kernel methods); <b>learning theory</b> (bias/variance tradeoffs; VC theory; large margins); <b>reinforcement learning and adaptive control</b>. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.</p>

<h2><b> â™ž INTRODUCTION </b></h2>

ð“€½ Basic concepts.

<h2><b> â™ž SUPERVISED LEARNING </b></h2>

ð“€½ Supervised learning setup. LMS. <br/>
ð“€½ Logistic regression. Perceptron. Exponential family. <br/>
ð“€½ Generative learning algorithms. Gaussian discriminant analysis. Naive Bayes. <br/>
ð“€½ Support vector machines. <br/>
ð“€½ Model selection and feature selection. <br/>
ð“€½ Evaluating and debugging learning algorithms. <br/>

<h2><b> â™ž LEARNING THEORY </b></h2>

ð“€½ Bias/variance tradeoff. <br/>
ð“€½ Practical advice on how to use learning algorithms. <br/>

<h2><b> â™ž DEEP LEARNING </b></h2>

ð“€½ Neural Networks. Backpropagation. <br/>
ð“€½ Vectorization. <br/>

<h2><b> â™ž UNSUPERVISED LEARNING </b></h2>

ð“€½ Clustering. K-means. <br/>
ð“€½ EM. Mixture of Gaussians. <br/>
ð“€½ Factor analysis. <br/>
ð“€½ PCA (Principal components analysis). <br/>
ð“€½ ICA (Independent components analysis). <br/>

<h2><b> â™ž REINFORCEMENT LEARNING AND CONTROL </b></h2>

ð“€½ MDPs. Bellman equations. <br/>
ð“€½ Value iteration and policy iteration. Linear quadratic regulation (LQR). LQG. <br/>
ð“€½ Q-learning. Value function approximation. Policy search. Reinforce. POMDPs. <br/>


[Exams]() | [MIT ML](http://machinelearning.mit.edu/) | [ML Cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/) | [CS229 Cheatsheet](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning) | [Google Scholar](https://scholar.google.co.in/) | [arXiv](https://arxiv.org/) | [UCI ML dataset repository](http://archive.ics.uci.edu/ml/index.php)

[Official Notes](https://github.com/SKKSaikia/CS229_ML/tree/master/doc/cs229-notes-all) : [compiled](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/CS229.pdf) | [CS229 Notes by Shervine](https://stanford.edu/~shervine/teaching/cs-229.html) | [Advice on applying ML - Andrew NG](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/ML-advice.pdf) | [Machine learning study guides tailored to CS 229](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning) | [Derivatives Backpropagation and Vectorization](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/Derivatives%20Backpropagation%20and%20Vectorization.pdf) | [CS229 More Notes](https://github.com/SKKSaikia/CS229_ML/tree/master/doc/NOTES) - [Section Notes](https://github.com/SKKSaikia/CS229_ML/tree/master/doc/NOTES/section_notes) - [Supplementary Notes](https://github.com/SKKSaikia/CS229_ML/tree/master/doc/NOTES/supplementary_notes)

ð“„† <b>[Cheatsheets](https://github.com/SKKSaikia/CS229_ML/tree/master/doc/cheatsheets) : </b><br/>
ð“€¯ [Supervised Learning](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/cheatsheet-supervised-learning.pdf)
ð“€¯ [Unsupervised Learning](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/cheatsheet-unsupervised-learning.pdf)
ð“€¯ [Machine Learning](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/cheatsheet-machine-learning-tips-and-tricks.pdf)
ð“€¯ [Deep Learning](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/cheatsheet-deep-learning.pdf)
ð“€¯ [Algebra Calculus](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/refresher-algebra-calculus.pdf)
ð“€¯ [Probability Statistics](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/refresher-probabilities-statistics.pdf)
ð“€¯ [Combined](https://github.com/SKKSaikia/CS229_ML/blob/master/doc/cheatsheets/super-cheatsheet-machine-learning.pdf)

# FINAL PROJECT

CS229 gives a lot of importance to Final Project, going through the [past projects](http://cs229.stanford.edu/projects.html) , I was happy to see the dynamic range of wonderful ideas and application of ML all the way. Good CS229 projects are either publishable or minor changes to be able to publish the project. [NIPS (NeurIPS)](https://nips.cc/) , [ICML](https://icml.cc/) are the ML conferences to show ML works to other people around the world. I was ecstatic to start my own and did " ".
